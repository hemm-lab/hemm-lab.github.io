<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="description" content="Holistic Evaluation of Multimodal foundational models">
        <meta name="keywords" content="HEMM, Multimodal Evaluation, Multimodal Benchmark">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>HEMM: Holistic Evaluation of Multimodal Models</title>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
        <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
        </script>
        <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
        <link rel="stylesheet" href="./static/css/bulma.min.css">
        <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
        <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        <link rel="stylesheet" href="./static/css/index.css">
        <link rel="icon" href="./static/images/favicon.png">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>
        <script src="./static/js/bulma-carousel.min.js"></script>
        <script src="./static/js/bulma-slider.min.js"></script>
        <script src="./static/js/index.js"></script>
    </head>
    <body>
        <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://www.cs.cmu.edu/~pliang/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->
        <section class="hero">
            <div class="hero-body">
                <div class="container is-max-desktop">
                    <div class="columns is-centered">
                        <div class="column has-text-centered">
                            <h1 class="title is-1 publication-title">HEMM: Holistic Evaluation of Multimodal Foundation Models</h1>
                            <div class="is-size-5 publication-authors">
                                <span class="author-block">
                                    <a href="https://www.cs.cmu.edu/~pliang/">Paul Liang</a>
                                    <sup>1</sup>,
                                </span>
                                <span class="author-block">
                                    <a href="https://akshayg08.github.io/">Akshay Goindani</a>
                                    <sup>1</sup>,
                                </span>
                                <span class="author-block">
                                    <a href="https://www.linkedin.com/in/talhachafekar/">Talha Chafekar</a>
                                    <sup>2</sup>
                                    ,
                                </span>
                                <span class="author-block">
                                    <a href="https://haofeiyu.me/">Haofei Yu</a>
                                    <sup>1</sup>
                                    ,
                                </span>
                                <span class="author-block">
                                    <a href="https://l-mathur.github.io/">Leena Mathur</a>
                                    <sup>1</sup>
                                    ,
                                </span>
                                <!-- <span class="author-block">
              <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
            </span> -->
                            </div>
                            <div class="is-size-5 publication-authors">
                                <span class="author-block">
                                    <sup>1</sup>Carnegie Mellon University
                                </span>
                                <span class="author-block">
                                    <sup>2</sup>KJSCE
                                </span>
                            </div>
                            <div class="column has-text-centered">
                                <div class="publication-links">
                                    <!-- PDF Link. -->
                                    <span class="link-block">
                                        <a href="" class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fas fa-file-pdf"></i>
                                            </span>
                                            <span>Paper</span>
                                        </a>
                                    </span>
                                    <span class="link-block">
                                        <a href="" class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="ai ai-arxiv"></i>
                                            </span>
                                            <span>arXiv</span>
                                        </a>
                                    </span>
                                    <!-- Video Link. -->
                                    <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
                                    <!-- Code Link. -->
                                    <span class="link-block">
                                        <a href="https://github.com/pliang279/HEMM" class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fab fa-github"></i>
                                            </span>
                                            <span>Code</span>
                                        </a>
                                    </span>
                                    <!-- Dataset Link. -->
                                    <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->
        <!-- 
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
        <section class="section hero is-light">
            <div class="container is-max-desktop">
                <!-- Abstract. -->
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Abstract</h2>
                        <div class="content has-text-justified">
                            <p>
                                Multimodal foundation models that can holistically process text alongside images, video, audio, and sensory modalities are increasingly becoming leveraged in a range of real-world domains. It is challenging to characterize and study progress in multimodal foundation modeling, given the range of possible modeling decisions, tasks, and downstream domains. In this paper, we introduce Holistic Evaluation of Multimodal Models (HEMM) as a framework to systematically evaluate the capabilities of multimodal foundation models across a set of 3 comprehensive dimensions: basic skills, information flow, and real-world use cases. Basic skills are  internal multimodal abilities required to solve problems, such as learning interactions, fine-grained alignment, multi-step reasoning, and abilities to handle external knowledge. Information flow studies how multimodal content changes during a task through querying, translation, editing, and fusion. 
            Usecases span  domain-specific challenges introduced in real-world multimedia, affective computing, natural sciences, healthcare, and interactive agent applications.
            Overall, HEMM includes 29 datasets and enables a comprehensive evaluation of multimodal models. 
            Using HEMM, our results (1) summarize the performance of individual models across multimodal tasks, and (2) distill broader performance trends regarding different design decisions in multimodal models (e.g., scale, frozen unimodal encoders, adapters, instruction tuning). Our experiments suggest the following promising research directions for the community TODO. HEMM is publicly-available at \url{anon} and encourages community involvement in its expansion.
                            </p>
                            <!-- <p>
            Our approach augments neural radiance fields
            (NeRF) by optimizing an
            additional continuous volumetric deformation field that warps each observed point into a
            canonical 5D NeRF.
            We observe that these NeRF-like deformation fields are prone to local minima, and
            propose a coarse-to-fine optimization method for coordinate-based models that allows for
            more robust optimization.
            By adapting principles from geometry processing and physical simulation to NeRF-like
            models, we propose an elastic regularization of the deformation field that further
            improves robustness.
          </p>
          <p>
            We show that <span class="dnerf">Nerfies</span> can turn casually captured selfie
            photos/videos into deformable NeRF
            models that allow for photorealistic renderings of the subject from arbitrary
            viewpoints, which we dub <i>"nerfies"</i>. We evaluate our method by collecting data
            using a
            rig with two mobile phones that take time-synchronized photos, yielding train/validation
            images of the same pose at different viewpoints. We show that our method faithfully
            reconstructs non-rigidly deforming scenes and reproduces unseen views with high
            fidelity.
          </p> -->
                        </div>
                    </div>
                </div>
            </div>
        </section>

        
        <section class="section">
            <div class="container is-max-desktop"></div>
            <!--/ Abstract. -->
            <!-- HEMM image. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">HEMM Overview</h2>
                    <div class="content has-text-justified">
                        <p>
                            HEMM is an evaluation framework that characterizes multimodal models by several dimensions (size, architecture, pretraining objective, fine-tuning objective, training data) and emphasizes holistic benchmarking of these models at
                            three disentangled levels: 
                            <ul>    
                                <li>
                                    <strong>Basic skills</strong><br>
                                    Benchmarking models' abilities to address multimodal problems, such as multimodal interactions, multimodal alignment, reasoning across compositional features, and integration of external knowledge.
                                </li>
                                <li>
                                    <strong>Information flow</strong><br>
                                    Benchmarking models' abilities to transform multimodal information during tasks such as querying, translation, editing, and fusion.
                                </li>
                                <li>
                                    <strong>Use cases</strong><br>
                                    Benchmarking models' abilities to perform in real-world problems from multimedia, affective computing, natural sciences, healthcare, and human-computer interaction.
                                </li>
                            </ul>
                        </p>
                    </div>
                    <div class="results-usecase">
                        <img src="static/hemm/hemm_overview.png" alt="Description of the image">
                        <p>Figure1. HEMM benchmark overview</p>
                    </div>
                </div>
            </div>
            <!--/ HEMM image. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <!-- <h2 class="title is-3">Results</h2> -->
                    <!-- <div class="results-usecase">
                  <img src="static/hemm/use_case.png" alt="Description of the image">
                  <p>Average performance of the models across use cases</p>
        </div>

        <div class="results-usecase">
                  <img src="static/hemm/num_total_params.png" alt="Description of the image">
                  <p>Performance of model vs number of parameters</p>
        </div> --></div>
            </div>

            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Key Challenges</h2>
                    <div class="content has-text-justified">
                        <p>
                            Based on the holistic evaluation of multimodal models in HEMM, we identify several key challenges that
                            multimodal models face in real-world applications. These challenges are as follows:
                        <ul>
                            <li><strong>Challenging Dataset</strong><br> Health, HCI and Science datasets are relatiely difficult
                                use cases for multimodal foundation models.
                            </li>
                            <li><strong>Multimodal Interactions</strong><br> Models perform better on redundant interactions but
                                struggle when visual information is not directly referenced by text.</li>
                            <li><strong>Reasoning, fine-grained, and knowledge</strong><br> We need better datasets that test for
                                complex reasoning and fine-grained alignment - current ones do not pose enough
                                challenges to today's models, with no significant performance differences with or without reasoning
                                and fine-grained
                                alignment.</li>
                            <li><strong>Model and data size</strong><br> Training on diverse data sources also improves over models
                                that only pretrain on images and captions. The tasks that
                                show the most improvement are iNaturalist and MemeCap which are knowledge intensive and require
                                complex reasoning.</li>
                            <li><strong>Model architecture and training</strong><br>Aligning frozen pre-trained language and vision
                                models outperforms end-to-end multimodal learning, and
                                instruction-tuned models performed better than those with only supervised fine-tuning</li>
                        </ul>
                        </p>
                    </div>
                </div>
            </div>

        </section>

        <section class="section hero is-light">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Multimodal Models in HEMM</h2>
                </div>
            </div>
        </section>

        <section class="section">


            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <div class="content has-text-justified">
                        <p>
                            In HEMM, <strong>11</strong> multimodal models with different model architecture and training methods are included. These models are considered as state-of-the-art in multimodal research and are evaluated across the three levels of HEMM. The full list of models is as follows (ranking from small-size ones to large-size ones):
                            <ul>
                                <li>KOSMOS-2</li>
                                <li>Open-Flamingo</li>
                                <li>Instruct-BLIP</li>
                                <li>Llama-Adapter</li>
                                <li>mPLUG-OWL</li>
                                <li>Fuyu-8B</li>
                                <li>BLIP-2</li>
                                <li>mini-GPT-4</li>
                                <li>EMU</li>
                                <li>Gemini</li>
                                <li>GPT-4-vision</li>
                        </p>
                        <p>
                            TODO: add a radial plot.
                        </p>
                    </div>
                </div>
            </div>

        </section>
        
        <section class="section hero is-light">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Multimodal Dataset in HEMM</h2>
                </div>
            </div>
        </section>
        
        <section class="section">

            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <div class="content has-text-justified">
                        <p>
                            HEMM includes <strong>30</strong> multimodal datasets that span a wide range of domains and tasks. These datasets are used to evaluate the performance of multimodal models across the three levels of HEMM. The full list of datasets grouped by their use case is as follows:
                            <ul>
                                <li><strong>Multimedia</strong><br> Winoground, IRFL, NLVR2, GQA, NLVR, Nocaps, VCR, Flickr30k, OK-VQA, VisualGenome, MM-IMDb, VQA v1
                                </li>
                                <li><strong>Affect</strong><br> Hateful Memes, Face Emotion, Memotion, MemeCap, New York Cartoon </li>
                                <li><strong>Science</strong><br>RESISC45, UC Merced, iNaturalist, Decimer, ScienceQA</li>
                                <li><strong>Health</strong><br> OpenPath, VQA-RAD, SLAKE, PathVQA</li>
                                <li><strong>HCI</strong><br> Screen2Words, Ethico</li>
                            </ul>
                        </p>
                    </div>
                    <div class="results-usecase">
                        <img src="static/hemm/dataset_use_case.png" alt="Description of the image">
                        <p>Figure3. Model performance on 30 datasets in HEMM grouped by their use case.</p>
                    </div>
                </div>
            </div>
        </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre>
                <code></code>
            </pre>
        </div>
    </section>
    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a> -->
                <a
                    class="icon-link"
                    href="https://github.com/pliang279"
                    class="external-link"
                    disabled
                >
                    <i class="fab fa-github"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            This website is licensed under a
                            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
                                Creative
            Commons Attribution-ShareAlike 4.0 International License
                            </a>
                            .
                        </p>
                        <p>
                            This means you are free to borrow the
                            <a href="https://github.com/nerfies/nerfies.github.io">source code</a>
                            of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>
</body>
</html>
